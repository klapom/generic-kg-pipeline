# üöÄ vLLM Konfiguration - Schnellstart

## üìÅ Wo Sie Konfigurationen eintragen m√ºssen

### 1. **Environment Variables (.env)**
```bash
# Kopieren Sie die Beispiel-Datei
cp .env.example .env

# Bearbeiten Sie diese Datei:
nano .env
```

**Wichtigste Einstellungen:**
```bash
# vLLM aktivieren
USE_VLLM=true

# GPU Memory (60-80% empfohlen)
VLLM_GPU_MEMORY_UTILIZATION=0.8

# Standardmodus (produktiv mit vLLM)
BATCH_DEFAULT_MODE=vllm

# Model-Cache-Verzeichnis (optional)
VLLM_MODEL_CACHE_DIR=/path/to/model/cache
```

### 2. **YAML-Konfiguration (config/default.yaml)**
```yaml
# vLLM Configuration
vllm:
  gpu_memory_utilization: 0.8  # GPU-Speichernutzung
  
  smoldocling:
    model_name: "ds4sd/SmolDocling-256M-preview"
    max_pages: 100
    extract_tables: true
    
  qwen25_vl:
    model_name: "Qwen/Qwen2.5-VL-7B-Instruct"
    max_image_size: 1024
    batch_size: 3
```

## üõ†Ô∏è Schnelle Installation

### 1. vLLM installieren
```bash
# Mit UV (empfohlen)
uv pip install vllm

# Zus√§tzliche Abh√§ngigkeiten
uv pip install pdf2image pillow
```

### 2. System-Abh√§ngigkeiten
```bash
# Ubuntu/Debian
sudo apt-get install poppler-utils

# macOS
brew install poppler
```

### 3. CUDA pr√ºfen
```bash
nvidia-smi
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

## üß™ Testen

### 1. Beispiel-Dokumente erstellen
```bash
python process_documents_vllm.py --create-samples
```

### 2. vLLM-Modus (mit GPU)
```bash
# Erst einzelne Datei testen
python process_documents_vllm.py --file sample_document.txt

# Vollst√§ndige Batch-Verarbeitung
python process_documents_vllm.py
```

## ‚öôÔ∏è Wichtige Konfigurationsoptionen

### GPU-Speicher anpassen
```bash
# In .env
VLLM_GPU_MEMORY_UTILIZATION=0.7  # F√ºr 8GB GPU
VLLM_GPU_MEMORY_UTILIZATION=0.8  # F√ºr 16GB+ GPU
```

### Batch-Performance optimieren
```bash
# In .env
BATCH_MAX_CONCURRENT=3        # F√ºr 16GB+ GPU
BATCH_MAX_CONCURRENT=2        # F√ºr 8GB GPU
BATCH_MAX_CONCURRENT=1        # F√ºr 4GB GPU
```

### Model-Pfade setzen (optional)
```bash
# In .env - f√ºr lokale Models
SMOLDOCLING_MODEL_PATH=/path/to/local/smoldocling
QWEN25_VL_MODEL_PATH=/path/to/local/qwen25-vl
```

## üö® H√§ufige Probleme & L√∂sungen

### "CUDA out of memory"
```bash
# GPU-Speicher reduzieren
VLLM_GPU_MEMORY_UTILIZATION=0.6
BATCH_MAX_CONCURRENT=1
```

### "Model not found"
```bash
# Model automatisch herunterladen lassen (Standard)
# Oder vorab herunterladen:
huggingface-cli download ds4sd/SmolDocling-256M-preview
```

### "vLLM not available"
```bash
# Installation pr√ºfen
pip install vllm --no-cache-dir
```

## üìä Performance-√úberwachung

### GPU-Status √ºberwachen
```bash
# W√§hrend der Verarbeitung
nvidia-smi -l 1
```

### Detaillierte Logs
```bash
# Verbose Modus
python process_documents_vllm.py --vllm --verbose
```

## üéØ Produktive Einstellungen

### F√ºr 8GB GPU
```bash
# .env
VLLM_GPU_MEMORY_UTILIZATION=0.7
BATCH_MAX_CONCURRENT=2
SMOLDOCLING_GPU_MEMORY=0.7
QWEN25_VL_GPU_MEMORY=0.6
```

### F√ºr 16GB+ GPU
```bash
# .env
VLLM_GPU_MEMORY_UTILIZATION=0.8
BATCH_MAX_CONCURRENT=3
SMOLDOCLING_GPU_MEMORY=0.8
QWEN25_VL_GPU_MEMORY=0.7
```

### F√ºr 24GB+ GPU
```bash
# .env
VLLM_GPU_MEMORY_UTILIZATION=0.9
BATCH_MAX_CONCURRENT=4
SMOLDOCLING_GPU_MEMORY=0.8
QWEN25_VL_GPU_MEMORY=0.8
```

## üîÑ Produktives Setup

1. **GPU-Setup pr√ºfen**
   ```bash
   nvidia-smi
   ```

2. **Schrittweise vLLM testen**
   ```bash
   # Erst einzelne Datei
   python process_documents_vllm.py --file sample_document.txt
   
   # Dann vollst√§ndige Batch
   python process_documents_vllm.py
   ```

3. **Konfiguration optimieren**
   ```bash
   # In .env
   VLLM_GPU_MEMORY_UTILIZATION=0.8
   BATCH_MAX_CONCURRENT=3
   ```

---

‚úÖ **Ihr vLLM-System ist bereit f√ºr 54.8% schnellere Dokumentenverarbeitung!**