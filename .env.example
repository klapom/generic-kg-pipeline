# Hochschul LLM Configuration
HOCHSCHUL_LLM_ENDPOINT=https://llm.hochschule.example/api
HOCHSCHUL_LLM_API_KEY=your-api-key-here

# vLLM SmolDocling Configuration (Legacy - now handled locally)
VLLM_SMOLDOCLING_URL=http://localhost:8002

# Storage Configuration
FUSEKI_URL=http://localhost:3030
CHROMADB_URL=http://localhost:8001

# Development LLM (Ollama)
OLLAMA_URL=http://localhost:11434

# ============================================================================
# vLLM Local Configuration
# ============================================================================

# vLLM General Settings
USE_VLLM=true
VLLM_GPU_MEMORY_UTILIZATION=0.8
VLLM_MAX_CONCURRENT_MODELS=2

# Model Cache Directory (optional - will download to default if not set)
VLLM_MODEL_CACHE_DIR=/path/to/model/cache

# SmolDocling Model Settings
SMOLDOCLING_MODEL_NAME=ds4sd/SmolDocling-256M-preview
SMOLDOCLING_MODEL_PATH=  # Leave empty to auto-download
SMOLDOCLING_MAX_PAGES=100
SMOLDOCLING_GPU_MEMORY=0.8

# Qwen2.5-VL Model Settings  
QWEN25_VL_MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct
QWEN25_VL_MODEL_PATH=  # Leave empty to auto-download
QWEN25_VL_GPU_MEMORY=0.7
QWEN25_VL_MAX_IMAGE_SIZE=1024

# Batch Processing Settings
BATCH_DEFAULT_MODE=vllm  # Production mode with vLLM
BATCH_MAX_CONCURRENT=3
BATCH_ENABLE_CHUNKING=true
BATCH_ENABLE_CONTEXT_INHERITANCE=true

# Performance Optimization
BATCH_AUTO_GPU_OPTIMIZATION=true
BATCH_MODEL_WARMUP=true
BATCH_CLEANUP_AFTER_BATCH=true

# Logging Configuration
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR
ENABLE_PERFORMANCE_LOGGING=true

# CUDA Settings (optional)
CUDA_VISIBLE_DEVICES=0  # Use GPU 0, or "0,1" for multiple GPUs
CUDA_MEMORY_FRACTION=0.9