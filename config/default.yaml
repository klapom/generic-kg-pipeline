# Domain Configuration
domain:
  name: "general"
  ontology_path: "plugins/ontologies/general.ttl"
  enabled_formats: ["pdf", "docx", "xlsx", "txt"]

# Document Parsing Configuration
parsing:
  pdf:
    provider: "vllm_smoldocling_local"  # Local vLLM SmolDocling
    max_pages: 100
    extract_tables: true
    extract_images: true
    extract_formulas: true
    preserve_layout: true
  
  office:
    provider: "native"
  
  text:
    provider: "native"

# LLM Configuration für Triple Extraction
llm:
  # Primärer Provider: Hochschul-LLM
  provider: "hochschul"
  
  hochschul:
    endpoint: "${HOCHSCHUL_LLM_ENDPOINT}"
    api_key: "${HOCHSCHUL_LLM_API_KEY}"
    model: "qwen1.5-72b"
    temperature: 0.1
    max_tokens: 4000
    timeout: 60
  
  # Fallback für lokale Entwicklung
  fallback_provider: "ollama"
  
  ollama:
    endpoint: "${OLLAMA_URL}"
    model: "qwen:7b"
    temperature: 0.1

# Chunking Configuration
chunking:
  max_tokens: 2000
  overlap_ratio: 0.2
  preserve_context: true

# Storage Configuration
storage:
  triple_store:
    type: "fuseki"
    endpoint: "${FUSEKI_URL}"
    dataset: "kg_dataset"
  
  vector_store:
    type: "chromadb"
    endpoint: "${CHROMADB_URL}"
    collection: "document_chunks"

# RAG Configuration
rag:
  similarity_threshold: 0.7
  max_context_chunks: 3
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"

# vLLM Configuration
vllm:
  # Global vLLM settings
  gpu_memory_utilization: 0.8
  max_concurrent_models: 2
  enable_model_caching: true
  
  # SmolDocling Model Settings
  smoldocling:
    model_name: "ds4sd/SmolDocling-256M-preview"
    model_path: null  # Set to local path if models are cached locally
    max_model_len: 16384  
    gpu_memory_utilization: 0.2
    trust_remote_code: true
    
    # SmolDocling-specific settings
    max_pages: 100
    extract_tables: true
    extract_images: true
    extract_formulas: true
    preserve_layout: true
    
    # Performance settings
    batch_size: 1
    timeout_seconds: 300
  
  # Qwen2.5-VL Model Settings
  qwen25_vl:
    model_name: "Qwen/Qwen2.5-VL-7B-Instruct"
    model_path: null  # Set to local path if models are cached locally
    max_model_len: 8192
    gpu_memory_utilization: 0.7  # Share GPU with SmolDocling
    trust_remote_code: true
    
    # Qwen2.5-VL specific settings
    max_image_size: 1024
    image_quality: 85
    batch_size: 3
    timeout_seconds: 60
    
    # Sampling settings
    temperature: 0.1
    max_tokens: 2000
    top_p: 1.0

# Batch Processing Configuration
batch_processing:
  default_mode: "vllm"  # Production mode with vLLM
  max_concurrent: 3
  timeout_seconds: 600
  
  # Enable/disable features
  enable_chunking: true
  enable_context_inheritance: true
  enable_visual_analysis: true
  
  # Performance optimization
  auto_gpu_memory_optimization: true
  model_warmup_enabled: true
  cleanup_after_batch: true