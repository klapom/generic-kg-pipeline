Projektanweisung
Projektziel
Entwicklung eines dom√§nen-agnostischen Document-to-Knowledge-Graph Systems mit 70% weniger Code als das bestehende Automotive-System, das RAG-Technologie integriert und horizontal skalierbar ist.
Kernvorgaben
Technische Anforderungen:

Sequenzielle GPU-Verarbeitung (keine Worker-Threads)
Plugin-basierte Architektur f√ºr Dom√§nen/Formate
Hybrid RAG+KG mit ChromaDB und Fuseki
Configuration-First Ansatz (YAML)
15 Kernmodule statt 50+

Phasenplan (4-5 Wochen):

MVP (1-2 Wochen): Core-Pipeline, Basic Parser, LLM-Integration
RAG (1 Woche): Vector Store, Hybrid Queries, Context-Enrichment
Production (2 Wochen): API, Monitoring, Docker, Skalierung

Deliverables:

Funktionierende Pipeline mit REST API
Docker-basiertes Deployment
Plugin-System f√ºr neue Dom√§nen
Performance-Monitoring
Dokumentation

Erfolgskriterien:

70%+ Performance-Steigerung bei Batch-Verarbeitung
Dom√§nen-Wechsel ohne Code-√Ñnderung
Horizontale Skalierung auf GPU-Nodes
<5s Query-Response f√ºr Hybrid-Suchen

Nicht im Scope:

Migration bestehender Automotive-Daten
Multi-Tenancy Features
Advanced Security/Compliance

Implementierungsrichtlinien
Arbeitsweise:

Schritt f√ºr Schritt denken
Komplexe Probleme in kleinere Probleme zerlegen
Immer im Internet nach Best Practices oder aktuellen Papers f√ºr eine Problemstellung suchen
Konzepte zun√§chst abstimmen, bevor Implementierung erfolgt

Technische Richtlinien:

Folge dem Implementierungsleitfaden und Best Practices Guide
Vermeide bekannte Anti-Patterns (Worker-Threads, √úber-Abstraktion)
Configuration-First vor Code-First Ansatz

Aktueller Implementierungsstand
‚úÖ VOLLST√ÑNDIG IMPLEMENTIERT (MVP-Phase):

Multi-Modal Document Parser System (100%)
- PDF Parser mit vLLM SmolDocling Integration
- DOCX Parser mit Image Extraction
- XLSX Parser mit Chart Analysis
- PPTX Parser mit Slide Visuals
- Parser Factory f√ºr automatische Format-Erkennung
- Context Mapping f√ºr pr√§zise Text-Bild-Zuordnung

Content Chunking mit Context Inheritance (100%)
- Structure-Aware Chunking basierend auf Dokumentstruktur
- Context Group Formation (PDF Sections, DOCX Headings, XLSX Sheets, PPTX Topics)
- LLM-basierte Context Summary Generation
- Dual-Task Prompting f√ºr optimale Kontext-Vererbung
- Async Processing f√ºr Performance

LLM Client Infrastructure (100%)
- vLLM SmolDocling Client f√ºr PDF-Parsing
- Hochschul-LLM Client f√ºr Triple Extraction
- Qwen2.5-VL Client f√ºr Visual Analysis
- OpenAI-kompatible API Integration

FastAPI Application (80%)
- Health, Documents, Pipeline, Query Endpoints
- Multi-Modal Upload Support
- Batch Processing Integration

Batch Processing System (100%)
- Concurrent Document Processing
- Progress Tracking und Error Handling
- Filesystem-basierte Verarbeitung

üîÑ IN ENTWICKLUNG:
- Triple Store Integration (Fuseki)
- Vector Store Integration (ChromaDB)
- End-to-End Pipeline Integration

Architektur-√úbersicht
Implementierte Komponenten-Struktur
core/                           # Kernmodule (implementiert)
‚îú‚îÄ‚îÄ content_chunker.py         # ‚úÖ Chunking mit Context Inheritance
‚îú‚îÄ‚îÄ batch_processor.py         # ‚úÖ Batch-Verarbeitung
‚îú‚îÄ‚îÄ config.py                  # ‚úÖ Unified Configuration
‚îú‚îÄ‚îÄ clients/                   # ‚úÖ LLM Client Infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ vllm_smoldocling.py   # ‚úÖ PDF-Parsing
‚îÇ   ‚îú‚îÄ‚îÄ hochschul_llm.py      # ‚úÖ Triple Extraction
‚îÇ   ‚îî‚îÄ‚îÄ qwen25_vl.py          # ‚úÖ Visual Analysis
‚îî‚îÄ‚îÄ chunking/                  # ‚úÖ Chunking System
    ‚îú‚îÄ‚îÄ chunk_models.py        # ‚úÖ Enhanced Data Models
    ‚îú‚îÄ‚îÄ context_grouper.py     # ‚úÖ Context Group Formation
    ‚îú‚îÄ‚îÄ context_summarizer.py  # ‚úÖ LLM-basierte Context Generation
    ‚îî‚îÄ‚îÄ base_chunker.py        # ‚úÖ Base Chunking Logic

plugins/                        # Plugin-Module (implementiert)
‚îú‚îÄ‚îÄ parsers/                   # ‚úÖ Multi-Modal Parser
‚îÇ   ‚îú‚îÄ‚îÄ pdf_parser.py         # ‚úÖ vLLM SmolDocling Integration
‚îÇ   ‚îú‚îÄ‚îÄ docx_parser.py        # ‚úÖ Image Extraction
‚îÇ   ‚îú‚îÄ‚îÄ xlsx_parser.py        # ‚úÖ Chart Analysis
‚îÇ   ‚îú‚îÄ‚îÄ pptx_parser.py        # ‚úÖ Slide Visuals
‚îÇ   ‚îú‚îÄ‚îÄ parser_factory.py     # ‚úÖ Automatic Format Selection
‚îÇ   ‚îî‚îÄ‚îÄ context_mapping.py    # ‚úÖ Enhanced Context Mapping
‚îú‚îÄ‚îÄ ontologies/                # Domain-Ontologien
‚îî‚îÄ‚îÄ templates/                 # ‚úÖ LLM-Prompt-Templates
    ‚îú‚îÄ‚îÄ context_generation.txt # ‚úÖ Context Inheritance Prompts
    ‚îî‚îÄ‚îÄ task_with_context.txt  # ‚úÖ Task Integration Prompts

api/                           # FastAPI Application (implementiert)
‚îú‚îÄ‚îÄ main.py                    # ‚úÖ Hauptanwendung
‚îî‚îÄ‚îÄ routers/                   # ‚úÖ API Endpoints
    ‚îú‚îÄ‚îÄ health.py             # ‚úÖ Health Checks
    ‚îú‚îÄ‚îÄ documents.py          # ‚úÖ Document Upload
    ‚îú‚îÄ‚îÄ pipeline.py           # ‚úÖ Pipeline Processing
    ‚îî‚îÄ‚îÄ query.py              # ‚úÖ Query Interface

config/                        # Configuration (implementiert)
‚îú‚îÄ‚îÄ default.yaml              # ‚úÖ System Configuration
‚îî‚îÄ‚îÄ chunking.yaml             # ‚úÖ Chunking Configuration
Datenfluss
Input Documents ‚Üí Document Parser ‚Üí Content Chunker ‚Üí RAG Processor
                                                           ‚îÇ
                                                           v
Vector Embedder ‚Üê Context Enricher ‚Üê LLM Client ‚Üê Triple Extractor
       ‚îÇ                                              ‚îÇ
       v                                              v
Vector Store ‚Üê Similarity Validator ‚Üí Triple Store ‚Üê RDF Writer
Kritische Lessons Learned
‚úÖ Best Practices (√úbernehmen)

Sequential Processing mit Model-Caching statt Worker-Threads
Context-Preserving Chunking f√ºr bessere LLM-Performance
Multi-Level Quality Validation f√ºr Triples
Domain-adaptive Prompt Engineering
Graceful Degradation und Fallback-Strategien

‚ùå Anti-Patterns (Vermeiden)

Worker-Thread-Architektur bei GPU-Modellen
Hybrid Live/Batch System (unn√∂tige Komplexit√§t)
√úber-abstrahierte Router-Hierarchie
Inkonsistente Datenstrukturen
Premature Optimization

Performance-Optimierungen
Model-Caching
python# ‚úÖ Empfohlen: Einmaliges Model-Loading
def process_sequential_cached(documents):
    switch_to_optimized_smoldocling()  # Einmal laden
    results = []
    for doc in documents:
        result = process_file(doc)  # Model wird wiederverwendet
        results.append(result)
    cleanup_smoldocling_model()
    return results
GPU-bewusste Skalierung

Separate Nodes statt Worker-Threads
Ein GPU-Model pro Node statt Model pro Thread
Sequenzielle Verarbeitung pro Node
Message Queue zwischen Nodes statt Shared Memory

Token-Limit-Management
Optimale Chunking-Konfiguration
yamlpdf_documents:
  max_tokens: 2000
  overlap_ratio: 0.2
  preserve_structure: true

office_documents:
  max_tokens: 1500
  overlap_ratio: 0.15
  preserve_tables: true

web_content:
  max_tokens: 1000
  overlap_ratio: 0.3
  clean_html: true
RAG-Integration
Hybrid-Ansatz
Vector Store ‚Üê‚Üí Knowledge Graph
     ‚Üë               ‚Üë
Semantic Search ‚Üê‚Üí SPARQL Queries
     ‚Üì               ‚Üì
  Similarity ‚Üê‚Üí Structured Facts
Implementierungsstrategie

ChromaDB f√ºr Vector Storage
Hybrid Queries (Vector + SPARQL)
Context-Enrichment f√ºr LLM
Advanced Similarity-basierte Validation

Entwicklungshinweise
Phase 1: MVP

Core Pipeline mit Document Parser, Content Chunker, LLM Client, KG Store
Basic Text/PDF Parsing (ohne SmolDocling zun√§chst)
Ollama LLM Integration mit Qwen:7b
Fuseki Triple Store Setup
CLI Interface

Phase 2: RAG-Integration

ChromaDB Vector Store Integration
Semantic Similarity Search
Hybrid Query Engine
Context-Enrichment f√ºr Triple-Extraktion
Quality Validation mit Similarity

Phase 3: Production Features

FastAPI REST API
Domain Plugin System
SmolDocling PDF Parser
Performance Monitoring
Docker Deployment
Horizontale Skalierung

Konfigurationsbeispiel
yamldomain:
  name: "general"
  ontology_path: "plugins/ontologies/general.ttl"
  enabled_formats: ["pdf", "docx", "txt"]

chunking:
  max_tokens: 2000
  overlap_ratio: 0.2
  preserve_context: true

llm:
  provider: "ollama"
  model: "qwen:7b"
  temperature: 0.1

storage:
  triple_store_url: "http://localhost:3030"
  vector_store_path: "data/vectors"